{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO, BytesIO\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extract():\n",
    "    def __init__(self, arg_date, src_format, src_bucket, trg_bucket, columns, key, s3, bucket):\n",
    "\n",
    "        # Parameters/Configurations\n",
    "        self.arg_date = arg_date\n",
    "        self.src_format = src_format\n",
    "        self.src_bucket = src_bucket\n",
    "        self.trg_bucket = trg_bucket\n",
    "        self.columns = columns\n",
    "        self.key = key\n",
    "        self.s3 = s3\n",
    "        self.bucket = bucket\n",
    "        \n",
    "    def read_csv_to_df(self, filename,bucket):\n",
    "        csv_obj = bucket.Object(key=filename).get().get('Body').read().decode('utf-8')\n",
    "        data = StringIO(csv_obj)\n",
    "        df = pd.read_csv(data, delimiter=',')\n",
    "        return df\n",
    "\n",
    "    def return_objects(self):\n",
    "        arg_date_dt = datetime.strptime(self.arg_date, self.src_format).date() - timedelta(days=1)\n",
    "        print(arg_date_dt)        \n",
    "        objects = [obj for obj in self.bucket.objects.all() if datetime.strptime(obj.key.split('/')[0], self.src_format).date() >= arg_date_dt]\n",
    "        return objects\n",
    "\n",
    "    def extract(self, objects,columns,bucket):\n",
    "        df_all = pd.concat([self.read_csv_to_df(obj.key, bucket) for obj in objects], ignore_index=True)\n",
    "        df_all = df_all.loc[:, columns]\n",
    "        df_all.dropna(inplace=True)\n",
    "        print(df_all)\n",
    "        return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform(Extract):\n",
    "    def transform_report(self, df_all):\n",
    "        df_all['opening_price'] = df_all.sort_values(by=['Time']).groupby(['ISIN', 'Date'])['StartPrice'].transform('first')\n",
    "        df_all['closing_price'] = df_all.sort_values(by=['Time']).groupby(['ISIN', 'Date'])['StartPrice'].transform('last')\n",
    "        df_all = df_all.groupby(['ISIN', 'Date'], as_index=False).agg(opening_price_eur=('opening_price', 'min'), closing_price_eur=('closing_price', 'min'), minimum_price_eur=('MinPrice', 'min'), maximum_price_eur=('MaxPrice', 'max'), daily_traded_volume=('TradedVolume', 'sum'))\n",
    "        df_all['prev_closing_price'] = df_all.sort_values(by=['Date']).groupby(['ISIN'])['closing_price_eur'].shift(1)\n",
    "        df_all['change_prev_closing_%'] = (df_all['closing_price_eur'] - df_all['prev_closing_price']) / df_all['prev_closing_price'] * 100\n",
    "        df_all.drop(columns=['prev_closing_price'], inplace=True)\n",
    "        df_all = df_all.round(decimals=2)\n",
    "        df_all = df_all[df_all.Date >= df_all]\n",
    "        print(df_all)\n",
    "        return df_all\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load():\n",
    "        \n",
    "    def write_df_to_s3(self, df_all,s3,trg_bucket,key):\n",
    "        out_buffer = BytesIO()\n",
    "        df_all.to_parquet(out_buffer, index=False)\n",
    "        bucket_target = s3.Bucket(trg_bucket)\n",
    "        bucket_target.put_object(Body=out_buffer.getvalue(), Key=key)\n",
    "        return bucket_target\n",
    "\n",
    "    def load(self, bucket_target):\n",
    "        objKey=[]\n",
    "        for obj in bucket_target.objects.all():\n",
    "            objKey.append(obj.key)\n",
    "        prq_obj = bucket_target.Object(key=objKey[-1]).get().get('Body').read()\n",
    "        data = BytesIO(prq_obj)\n",
    "        return data\n",
    "\n",
    "    def etl_report(self, bucket_target):\n",
    "        df_report = pd.read_parquet(self.load(bucket_target))\n",
    "        return df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    arg_date = '2022-04-07'\n",
    "    src_format = '%Y-%m-%d'\n",
    "    src_bucket = 'deutsche-boerse-xetra-pds'\n",
    "    trg_bucket = 'xetra-bucket-gerardo-2022'\n",
    "    columns = ['ISIN', 'Date', 'Time', 'StartPrice', 'MaxPrice', 'MinPrice', 'EndPrice', 'TradedVolume']\n",
    "    key = 'xetra_daily_report_' + datetime.today().strftime(\"%Y%m%d_%H%M%S\") + '.parquet'\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(src_bucket)\n",
    "\n",
    "    ext = Extract(arg_date, src_format, src_bucket, trg_bucket, columns, key, s3, bucket)\n",
    "    \n",
    "    objects = ext.return_objects()\n",
    "    df_all = ext.extract(objects,columns,bucket)\n",
    "    \n",
    "    tran = Transform()\n",
    "    df_all = tran.transform_report(df_all)\n",
    "    \n",
    "\n",
    "    ld = Load()\n",
    "    bucket_target = ld.write_df_to_s3(df_all,ext.s3,ext.trg_bucket,ext.key)\n",
    "    \n",
    "\n",
    "    print(ld.etl_report(bucket_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-06\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fc101cb3b0c590d49d7b75cf33e7dfc6fd6039ad56cc5f847ff14383603a395"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
